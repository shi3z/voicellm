<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width" />
  <title>Next-gen Kaldi WebAssembly with sherpa-onnx for VAD + ASR</title>
  <style>
    h1,div {
      text-align: center;
    }
    textarea {
      width:100%;
    }
    .loading {
      display: none !important;
    }
  </style>
</head>

<body style="font-family: 'Source Sans Pro', sans-serif; background-color: #f9fafb; color: #333; display: flex; flex-direction: column; align-items: center; height: 100vh; margin: 0;">
  <h1>Local LLM音声応答システム</h1>

  <div style="width: 100%; max-width: 900px; background: #fff; padding: 1.5rem; border-radius: 8px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); flex: 1;">
    <div id="status">Loading...</div>

    <div id="singleAudioContent" class="tab-content loading">
      <div style="display: flex; gap: 1.5rem;">
        <div style="flex: 1; display: flex; flex-direction: row; align-items: center; gap: 1rem;">
          <button id="startBtn" disabled>Start</button>
          <button id="stopBtn" disabled>Stop</button>
          <button id="clearBtn">Clear</button>
        </div>
      </div>

      <div style="flex: 1; display: flex; flex-direction: column; gap: 1rem;">
          <textarea id="results" rows="10" style="display:none;" placeholder="Output will appear here..." readonly style="flex: 1; padding: 0.75rem; font-size: 1rem; border: 1px solid #ced4da; border-radius: 8px; resize: none; background-color: #f8f9fa;"></textarea>
          <div id="conversation" style="text-align: left; width: 100%; max-height: 800px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 8px; background-color: #fafafa;"></div>
          
          <div style="display: flex; gap: 1rem; margin-top: 1rem;">
            <div id="speechStatus" style="padding: 0.75rem; font-size: 0.9rem; color: #6c757d; background-color: #f8f9fa; border-radius: 8px; flex: 1;"></div>
          </div>
      </div>

      <section flex="1" overflow="auto" id="sound-clips" style="display:none">
      </section>
  </div>


  <script src="sherpa-onnx-asr.js"></script>
  <script src="sherpa-onnx-vad.js"></script>
  <script src="app-vad-asr.js"></script>
  <script src="sherpa-onnx-wasm-main-vad-asr.js"></script>
  
  <!-- マークダウンパーサー -->
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  
  <script>
    // 音声合成機能の実装
    let speechSynth;
    let currentUtterance = null;
    let voices = [];
    
    // 音声合成の初期化
    function initSpeechSynthesis() {
      if ('speechSynthesis' in window) {
        speechSynth = window.speechSynthesis;
        
        // 音声一覧の取得（非同期）
        function loadVoices() {
          voices = speechSynth.getVoices();
          console.log('Available voices:', voices);
          
          // 日本語音声を探す
          const japaneseVoices = voices.filter(voice => 
            voice.lang.includes('ja') || voice.lang.includes('JP')
          );
          console.log('Japanese voices:', japaneseVoices);
        }
        
        // 音声一覧の読み込み完了を待つ
        if (voices.length === 0) {
          speechSynth.addEventListener('voiceschanged', loadVoices);
        } else {
          loadVoices();
        }
        
        return true;
      } else {
        console.error('Speech synthesis not supported');
        return false;
      }
    }
    
    // 音声応答ボタンのイベントリスナー
    document.addEventListener('DOMContentLoaded', function() {
      const speakBtn = document.getElementById('speakBtn');
      const speechStatus = document.getElementById('speechStatus');
      const results = document.getElementById('results');
      
      // 音声合成の初期化
      const speechSupported = initSpeechSynthesis();
      
      if (!speechSupported) {
        speechStatus.textContent = 'お使いのブラウザは音声合成に対応していません';
        speakBtn.disabled = true;
        return;
      }
      
      // 音声認識の制御
      let isRecording = false;
      
      function pauseRecording() {
        const stopBtn = document.getElementById('stopBtn');
        if (!stopBtn.disabled) {
          console.log('Pausing recording for speech synthesis');
          stopBtn.click(); // 音声認識を停止
          isRecording = true; // 再開フラグを設定
        }
      }
      
      function resumeRecording() {
        const startBtn = document.getElementById('startBtn');
        if (isRecording && !startBtn.disabled) {
          console.log('Resuming recording after speech synthesis');
          setTimeout(() => {
            startBtn.click(); // 音声認識を再開
            isRecording = false;
          }, 500); // 少し遅延を入れる
        }
      }

      // 英語部分を検出する関数
      function detectLanguageSegments(text) {
        // 英語のフレーズを検出する正規表現（英単語+スペースを含む）
        const englishRegex = /[a-zA-Z]+(?:[''][a-zA-Z]+)*(?:\s+[a-zA-Z]+(?:[''][a-zA-Z]+)*)*/g;
        const segments = [];
        let lastIndex = 0;
        let match;
        
        // 英語部分を検出
        while ((match = englishRegex.exec(text)) !== null) {
          // 英語の前の日本語部分
          if (match.index > lastIndex) {
            const japaneseText = text.substring(lastIndex, match.index).trim();
            if (japaneseText) {
              segments.push({ text: japaneseText, lang: 'ja-JP', type: 'japanese' });
            }
          }
          
          // 英語部分（スペースを含む）
          segments.push({ text: match[0], lang: 'en-US', type: 'english' });
          lastIndex = match.index + match[0].length;
        }
        
        // 最後の日本語部分
        if (lastIndex < text.length) {
          const remainingText = text.substring(lastIndex).trim();
          if (remainingText) {
            segments.push({ text: remainingText, lang: 'ja-JP', type: 'japanese' });
          }
        }
        
        // 英語が見つからない場合は全体を日本語として扱う
        if (segments.length === 0) {
          segments.push({ text: text.trim(), lang: 'ja-JP', type: 'japanese' });
        }
        
        return segments;
      }
      
      // 複数セグメントを順次再生する関数
      function speakSegments(segments, currentIndex = 0) {
        if (currentIndex >= segments.length) {
          console.log('All segments completed');
          speechStatus.textContent = 'AI応答完了';
          // 音声合成終了後に音声認識を再開
          resumeRecording();
          return;
        }
        
        const segment = segments[currentIndex];
        console.log(`Speaking segment ${currentIndex + 1}/${segments.length}: "${segment.text}" (${segment.lang})`);
        
        // 既存の音声を停止
        if (speechSynth.speaking || speechSynth.pending) {
          speechSynth.cancel();
        }
        
        const utterance = new SpeechSynthesisUtterance(segment.text);
        utterance.lang = segment.lang;
        
        // 言語に応じて音声を選択
        if (segment.type === 'english') {
          const englishVoice = voices.find(voice => 
            voice.lang.includes('en') || voice.lang.includes('US')
          );
          if (englishVoice) {
            utterance.voice = englishVoice;
            console.log('Using English voice:', englishVoice.name);
          }
        } else {
          const japaneseVoice = voices.find(voice => 
            voice.lang.includes('ja') || voice.lang.includes('JP')
          );
          if (japaneseVoice) {
            utterance.voice = japaneseVoice;
            console.log('Using Japanese voice:', japaneseVoice.name);
          }
        }
        
        utterance.rate = 0.9;
        utterance.pitch = 1.0;
        utterance.volume = 1.0;
        
        utterance.onend = function() {
          // 次のセグメントを再生
          setTimeout(() => speakSegments(segments, currentIndex + 1), 100);
        };
        
        utterance.onerror = function(event) {
          console.error('Speech error in segment:', event);
          // エラーが発生しても次のセグメントに進む
          setTimeout(() => speakSegments(segments, currentIndex + 1), 100);
        };
        
        speechSynth.speak(utterance);
      }

      // 音声応答機能
      function speakText(text, isApiResponse = false) {
        console.log('speakText called with:', text, 'isApiResponse:', isApiResponse);
        
        if (!text || text.trim() === '') {
          speechStatus.textContent = '応答するテキストがありません';
          return;
        }
        
        let textToSpeak = '';
        
        if (isApiResponse) {
          // APIレスポンスの場合はそのまま使用
          textToSpeak = text.trim();
        } else {
          // 音声認識結果からテキスト部分のみを抽出
          const lines = text.split('\n');
          
          for (const line of lines) {
            if (line.includes('Result:')) {
              const resultPart = line.split('Result:')[1];
              if (resultPart) {
                textToSpeak += resultPart.trim() + ' ';
              }
            }
          }
        }
        
        if (!textToSpeak.trim()) {
          speechStatus.textContent = isApiResponse ? 'AIレスポンスが空です' : '音声認識結果がありません';
          return;
        }
        
        console.log('Text to speak:', textToSpeak);
        
        // 音声合成開始前に音声認識を停止
        pauseRecording();
        
        // 既存の音声を停止
        if (speechSynth.speaking || speechSynth.pending) {
          console.log('Cancelling previous speech');
          speechSynth.cancel();
        }
        
        // 言語セグメントに分割
        const segments = detectLanguageSegments(textToSpeak.trim());
        console.log('Language segments:', segments);
        
        speechStatus.textContent = isApiResponse ? 'AI応答中（録音停止）...' : '音声応答中（録音停止）...';
        
        // セグメントを順次再生
        speakSegments(segments);
      }
      
      // 音声認識結果が更新された時の処理
      let lastText = '';
      
      // テキストエリアの変更を監視
      const textAreaObserver = new MutationObserver(function(mutations) {
        const currentText = results.value;
        if (currentText !== lastText && currentText.trim() !== '') {
          console.log('Text area updated:', currentText);
          lastText = currentText;
          speechStatus.textContent = '音声応答準備完了';
        }
      });
      
      // テキストエリアのinputイベントも監視
      results.addEventListener('input', function() {
        const currentText = results.value;
        if (currentText !== lastText && currentText.trim() !== '') {
          console.log('Input event:', currentText);
          lastText = currentText;
          speechStatus.textContent = '音声応答準備完了';
        }
      });
      
      // マークダウンをHTMLに変換する関数
      function parseMarkdown(text) {
        if (typeof marked !== 'undefined') {
          // markedライブラリが利用可能な場合
          return marked.parse(text);
        } else {
          // フォールバック: 簡単なマークダウン処理
          return text
            .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
            .replace(/\*(.*?)\*/g, '<em>$1</em>')
            .replace(/`(.*?)`/g, '<code>$1</code>')
            .replace(/\n/g, '<br/>');
        }
      }

      // Python API サーバー呼び出し機能
      async function callPythonAPI(userInput) {
        var conversation = document.getElementById('conversation');
        try {
          speechStatus.textContent = 'AIに問い合わせ中...';
          conversation.innerHTML += "<div style='margin: 10px 0; padding: 10px; background: #e3f2fd; border-radius: 5px;'><B>USER:</B> <span style='color: #1976d2;'>" + userInput + "</span></div>";
          
          const response = await fetch('/api/chat', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              message: userInput
            })
          });

          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
          }

          const data = await response.json();
          
          if (data.status === 'success') {
            const aiResponse = data.response;
            console.log('Python API response:', aiResponse);
            
            // マークダウンをHTMLに変換
            const htmlResponse = parseMarkdown(aiResponse);
            
            // APIレスポンスを表示
            conversation.innerHTML += "<div style='margin: 10px 0; padding: 10px; background: #f1f8e9; border-radius: 5px;'><B>AI:</B> <div style='color: #388e3c; margin-top: 5px;'>" + htmlResponse + "</div></div>";
            
            // 音声合成では元のテキストを使用（HTMLタグなし）
            speakText(aiResponse, true);
            
            // 会話履歴の最下部にスクロール
            conversation.scrollTop = conversation.scrollHeight;
          } else {
            throw new Error(data.error || 'Unknown error');
          }
          
        } catch (error) {
          console.error('Python API error:', error);
          speechStatus.textContent = 'API接続エラー: ' + error.message;
          
          // エラー時は元のテキストをそのまま読み上げ
          speakText(`${userInput}について、理解しました。`, true);
        }
      }

      // PropertyObserver（テキストエリアの値の変更を検知）
      let lastValue = '';
      setInterval(function() {
        if (results.value !== lastValue) {
          lastValue = results.value;
          console.log("音声認識結果:", lastValue)
          
          // 音声認識結果からテキストを抽出
          const lines = lastValue.split('\n');
          let recognizedText = '';
          for (const line of lines) {
            if (line.includes('Result:')) {
              const resultPart = line.split('Result:')[1];
              if (resultPart) {
                recognizedText += resultPart.trim() + ' ';
              }
            }
          }
          
          if (recognizedText.trim()) {
            // Python APIサーバーを呼び出し
            callPythonAPI(recognizedText.trim());
          }
          
          results.value="";
          const clearBtn = document.getElementById('clearBtn');
          clearBtn.click();

          if (lastValue.trim() !== '') {
            speechStatus.textContent = '音声応答準備完了';
          }
        }
      }, 500);
      
      // 初期状態設定
      speechStatus.textContent = '音声合成を初期化しています...';
      
      // 初期化完了後の処理
      setTimeout(() => {
        if (speechSynth && speechSynth.getVoices().length > 0) {
          speechStatus.textContent = '音声認識を開始してください';
        } else {
          speechStatus.textContent = '音声合成の初期化中...';
        }
      }, 1000);
    });
  </script>
</body>
